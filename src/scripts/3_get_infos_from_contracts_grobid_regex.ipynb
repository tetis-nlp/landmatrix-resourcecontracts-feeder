{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliothèques à installer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/kermitt2/grobid_client_python.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install grobid-client-python\n",
    "!pip install requests\n",
    "!pip install ocrmypdf\n",
    "!pip install PyPDF2\n",
    "!pip install grobid-client\n",
    "!pip install beautifulsoup4\n",
    "!pip install PyMuPDF\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour convertir les pdfs scannés en pdfs exportés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ocrmypdf\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def pdf_has_text(pdf_path, threshold=10):\n",
    "    \"\"\"Check if a PDF file has text content greater than threshold length.\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "            if len(text) > threshold:\n",
    "                return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        \n",
    "        return False\n",
    "\n",
    "def process_pdfs_in_folder(folder_path, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            input_pdf = os.path.join(folder_path, filename)\n",
    "            output_pdf = os.path.join(output_folder, filename)\n",
    "            \n",
    "            print(f\"Processing {filename}...\")\n",
    "            if pdf_has_text(input_pdf):\n",
    "                print(\" -> Already searchable, skipping OCR.\")\n",
    "                \n",
    "                if input_pdf != output_pdf:\n",
    "                    import shutil\n",
    "                    shutil.copy2(input_pdf, output_pdf)\n",
    "            else:\n",
    "                print(\" -> No text detected, running OCR...\")\n",
    "                try:\n",
    "                    ocrmypdf.ocr(input_pdf, output_pdf, use_threads=True)\n",
    "                    print(f\" --> OCR done: {output_pdf}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during OCR for {filename}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder = \"PDFs_Extraction\\contrats\"\n",
    "    output = \"PDFs_Extraction\\contrats_scannées\"\n",
    "    process_pdfs_in_folder(folder, output)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour extraire les infos des pdfs scannés en utilisant GROBID + Regex & Keyword-Based Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Paths\n",
    "PDF_FOLDER = \"PDFs_Extraction/contrats_scannées\"\n",
    "COUNTRY_LIST_FILE = \"PDFs_Extraction/countries_fr.txt\"\n",
    "GROBID_URL = \"http://localhost:8070/api/processFulltextDocument\"\n",
    "OUTPUT_JSON = \"extracted_contract_info.json\"\n",
    "\n",
    "# Company Keywords\n",
    "company_keywords = [\n",
    "    \"Ltd\", \"Inc\", \"Corp\", \"LLC\", \"Company\", \"Corporation\", \"Enterprises\", \"Group\",\n",
    "    \"Holdings\", \"Services\", \"Solutions\", \"Industries\", \"Systems\", \"Technologies\",\n",
    "    \"Partners\", \"Consulting\", \"Management\", \"Trading\", \"Operations\",\n",
    "    \"SARL\", \"SA\", \"SAS\", \"SNC\", \"Entreprise\", \"Compagnie\", \"Société\", \"Groupe\",\n",
    "    \"Conseil\", \"Commerce\", \"Gestion\", \"Opérations\", \"Sprl\", \"Office\"\n",
    "]\n",
    "\n",
    "# Encoding Fix\n",
    "def fix_encoding(text: str) -> str:\n",
    "    try:\n",
    "        return text.encode(\"latin1\").decode(\"utf-8\")\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "# Load Country List\n",
    "def load_country_list(file_path: str) -> list:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "# Improved Country Extraction Using Regex Word Boundaries\n",
    "def extract_countries(text: str, country_list: list) -> list:\n",
    "    found_countries = set()\n",
    "    text = text.lower()\n",
    "    for country in country_list:\n",
    "        pattern = r'\\b' + re.escape(country) + r'\\b'\n",
    "        if re.search(pattern, text):\n",
    "            found_countries.add(country)\n",
    "    return list(found_countries)\n",
    "\n",
    "# Extract First Page Text with PyMuPDF\n",
    "def extract_first_page_text_with_pymupdf(pdf_path: str) -> str:\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        if len(doc) == 0:\n",
    "            return \"\"\n",
    "        first_page = doc.load_page(0)\n",
    "        return first_page.get_text().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text with PyMuPDF from {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Extract Title Based on Keywords\n",
    "def extract_title_from_text(text: str) -> str:\n",
    "    lines = text.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        l = line.lower()\n",
    "        if any(keyword in l for keyword in [\n",
    "            \"contrat\", \"convention\", \"accord\", \"protocole\", \"entente\",\n",
    "            \"type d’amodiation\", \"amodiation\", \"licence\", \"autorisation\",\n",
    "            \"deal\", \"memorandum\", \"type d'accord\", \"type de contrat\"\n",
    "        ]):\n",
    "            return line.strip()\n",
    "    return lines[0].strip() if lines else \"\"\n",
    "\n",
    "# Extract Company Name\n",
    "def extract_company_name(filename: str, text: str) -> str:\n",
    "    candidates = []\n",
    "\n",
    "    # Check filename\n",
    "    for keyword in company_keywords:\n",
    "        pattern = r'([A-Z][\\w&., -]+?\\b' + re.escape(keyword) + r'\\b)'\n",
    "        match = re.search(pattern, filename, re.IGNORECASE)\n",
    "        if match:\n",
    "            candidates.append(match.group(0).strip())\n",
    "\n",
    "    # Check text\n",
    "    for keyword in company_keywords:\n",
    "        pattern = r'([A-Z][\\w&., -]+?\\b' + re.escape(keyword) + r'\\b)'\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            candidates.append(match.group(0).strip())\n",
    "\n",
    "    return candidates[0] if candidates else \"\"\n",
    "\n",
    "# Extract Info from PDF (uses both GROBID and PyMuPDF)\n",
    "def extract_contract_info_from_pdf(pdf_path: str, country_list: list):\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as pdf_file:\n",
    "            response = requests.post(GROBID_URL, files={\"input\": pdf_file})\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to process {pdf_path}\")\n",
    "            return None\n",
    "\n",
    "        raw_xml = response.text\n",
    "        raw_text = fix_encoding(BeautifulSoup(raw_xml, \"lxml\").get_text())\n",
    "\n",
    "        # First page\n",
    "        first_page_text = extract_first_page_text_with_pymupdf(pdf_path)\n",
    "\n",
    "        title = extract_title_from_text(first_page_text)\n",
    "        countries = extract_countries(raw_text, country_list)\n",
    "        company_name = extract_company_name(os.path.basename(pdf_path), first_page_text)\n",
    "\n",
    "        return {\n",
    "            \"file_name\": os.path.basename(pdf_path),\n",
    "            \"titre_contrat\": title,\n",
    "            \"pays_cible\": countries,\n",
    "            \"societe_exploitation\": company_name\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main Routine\n",
    "def main():\n",
    "    country_list = load_country_list(COUNTRY_LIST_FILE)\n",
    "    results = []\n",
    "\n",
    "    for filename in os.listdir(PDF_FOLDER):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            print(f\"Processing {filename}...\")\n",
    "            pdf_path = os.path.join(PDF_FOLDER, filename)\n",
    "            info = extract_contract_info_from_pdf(pdf_path, country_list)\n",
    "            if info:\n",
    "                results.append(info)\n",
    "\n",
    "    if results:\n",
    "        with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\nExtraction complete. Data saved to: {OUTPUT_JSON}\")\n",
    "    else:\n",
    "        print(\"\\nNo contracts extracted.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
