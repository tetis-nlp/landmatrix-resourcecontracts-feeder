{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Bibliothèques à télécharger**"
      ],
      "metadata": {
        "id": "XcNUpa3Aq-xi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaNYUedZJs7P"
      },
      "outputs": [],
      "source": [
        "!pip install pymupdf\n",
        "!pip install pytesseract\n",
        "!pip install pdf2image\n",
        "!pip install easyocr\n",
        "!apt-get install -y poppler-utils\n",
        "!pip install pillow\n",
        "!pip install spacy\n",
        "!pip install tqdm\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install accelerate\n",
        "!pip install transformers torch spacy\n",
        "!python -m spacy download fr_core_news_md"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code 1 : Extraction des textes à partir des pdfs**"
      ],
      "metadata": {
        "id": "1KoXnh8tq5I0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "from pdf2image import convert_from_path\n",
        "import easyocr\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "input_folder = \"contracts_pdf\"\n",
        "output_folder = \"extracted_text\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "reader = easyocr.Reader(['fr'], gpu=False)\n",
        "\n",
        "def is_scanned_page(text: str) -> bool:\n",
        "    return len(text.strip()) < 30  # Heuristique simple\n",
        "\n",
        "def remove_bottom(image: Image.Image, percentage=20) -> Image.Image:\n",
        "    \"\"\"Supprimer le bas de l’image (ex. : pour ignorer signatures/cachets)\"\"\"\n",
        "    width, height = image.size\n",
        "    crop_height = height * (100 - percentage) // 100\n",
        "    cropped = image.crop((0, 0, width, crop_height))\n",
        "    return cropped\n",
        "\n",
        "def clean_text(lines):\n",
        "    \"\"\"Supprime les lignes très courtes ou vides\"\"\"\n",
        "    cleaned = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if len(line) < 5:\n",
        "            continue\n",
        "        cleaned.append(line)\n",
        "    return cleaned\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    text_result = \"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        for i in range(len(doc)):\n",
        "            page = doc[i]\n",
        "            text = page.get_text(\"text\")\n",
        "\n",
        "            if is_scanned_page(text):\n",
        "                # Page probablement scannée\n",
        "                print(f\"  OCR sur page {i + 1}\")\n",
        "                images = convert_from_path(pdf_path, first_page=i + 1, last_page=i + 1)\n",
        "                if images:\n",
        "                    image = remove_bottom(images[0], percentage=20)  # Retirer bas de page\n",
        "                    ocr_lines = reader.readtext(np.array(image), detail=0)\n",
        "                    lines = clean_text(ocr_lines)\n",
        "                    text_result += \"\\n\".join(lines) + \"\\n\"\n",
        "            else:\n",
        "                # Page textuelle\n",
        "                lines = clean_text(text.splitlines())\n",
        "                text_result += \"\\n\".join(lines) + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"[ERREUR] {pdf_path}: {e}\")\n",
        "    return text_result.strip()\n",
        "\n",
        "# Boucle sur les fichiers\n",
        "for file_name in os.listdir(input_folder):\n",
        "    if file_name.lower().endswith(\".pdf\"):\n",
        "        print(f\"Traitement de {file_name}...\")\n",
        "        pdf_path = os.path.join(input_folder, file_name)\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        if text:\n",
        "            output_path = os.path.join(output_folder, file_name.replace(\".pdf\", \".txt\"))\n",
        "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(text)\n",
        "            print(f\" Texte extrait vers {output_path}\")\n",
        "        else:\n",
        "            print(f\" Aucun texte trouvé pour {file_name}\")\n"
      ],
      "metadata": {
        "id": "S_EL6-cUqWaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code 2 : Extraction des infos à partir des textes en utilisant un modèle LLM : TinyLlama**"
      ],
      "metadata": {
        "id": "yzFdlsPArwuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Charger le modèle\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "company_keywords = [\n",
        "    # English\n",
        "    \"Ltd\", \"Inc\", \"Corp\", \"LLC\", \"Company\", \"Corporation\", \"Enterprises\",\n",
        "    \"Group\", \"Holdings\", \"Services\", \"Solutions\", \"Industries\",\n",
        "    \"Systems\", \"Technologies\", \"Partners\", \"Consulting\", \"Management\", \"Trading\", \"Operations\",\n",
        "\n",
        "    # French\n",
        "    \"SARL\", \"SA\", \"SAS\", \"SNC\", \"Entreprise\", \"Compagnie\", \"Société\",\n",
        "    \"Groupe\", \"Services\", \"Solutions\", \"Technologies\", \"Conseil\",\n",
        "    \"Commerce\", \"Gestion\", \"Opérations\", \"Sprl\"\n",
        "]\n",
        "\n",
        "# Découpe du texte en chunks de 1000 mots\n",
        "def split_text(text, max_words=1000):\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i + max_words]) for i in range(0, len(words), max_words)]\n",
        "\n",
        "# Charger la liste des pays depuis un fichier texte\n",
        "def load_country_list(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [line.strip() for line in f if line.strip()]\n",
        "\n",
        "def extract_company_name_from_text(response_text, keywords):\n",
        "    keyword_pattern = r'\\b(?:[A-Z][a-zA-Z&, \\'.-]+\\s)+(?:' + '|'.join(re.escape(k) for k in keywords) + r')\\b'\n",
        "    keyword_matches = re.findall(keyword_pattern, response_text)\n",
        "\n",
        "    caps_pattern = r'\\b(?:[A-Z]{2,}(?:\\s+[A-Z]{2,}){0,4})\\b'\n",
        "    caps_matches = re.findall(caps_pattern, response_text)\n",
        "\n",
        "    # Filter out short or generic words (optional: could be refined more)\n",
        "    caps_matches = [c.strip() for c in caps_matches if len(c) > 2 and not c.isdigit()]\n",
        "\n",
        "    all_matches = set(keyword_matches + caps_matches)\n",
        "    return list(all_matches)\n",
        "\n",
        "\n",
        "# Dossiers d'entrée/sortie\n",
        "text_folder = \"extracted_text\"\n",
        "output_folder = \"extracted_data\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Liste des pays en français\n",
        "country_list = load_country_list(\"countries_fr.txt\")\n",
        "\n",
        "# Traitement de chaque fichier texte dans le dossier\n",
        "for file_name in os.listdir(text_folder):\n",
        "    if file_name.endswith(\".txt\"):\n",
        "        file_path = os.path.join(text_folder, file_name)\n",
        "\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            content = f.read()\n",
        "\n",
        "        chunks = split_text(content, max_words=1000)\n",
        "        print(f\"\\nAnalyse du fichier : {file_name} ({len(chunks)} morceaux)\")\n",
        "\n",
        "        extracted_countries = []\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            country_prompt = f\"Quels sont les pays mentionnés dans ce texte : {chunk}\"\n",
        "            country_result = pipe(country_prompt, max_new_tokens=150, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "            country_response = country_result[0]['generated_text']\n",
        "\n",
        "            countries_found = []\n",
        "            for country in country_list:\n",
        "                # Regex pour éviter les faux positifs (mot entier uniquement)\n",
        "                pattern = r'\\b' + re.escape(country) + r'\\b'\n",
        "                if re.search(pattern, country_response, re.IGNORECASE):\n",
        "                    countries_found.append(country)\n",
        "\n",
        "            # Supprimer les doublons éventuels\n",
        "            countries_found = list(set(countries_found))\n",
        "\n",
        "            company_prompt =f\"Peux-tu extraire les noms des sociétés d'exploitation mentionné : {chunk}\"\n",
        "            company_result = pipe(company_prompt, max_new_tokens=150, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "            company_response = company_result[0]['generated_text']\n",
        "\n",
        "            company_names = extract_company_name_from_text(company_response, company_keywords)\n",
        "\n",
        "            extracted_countries.append({\n",
        "                \"chunk\": i + 1,\n",
        "                \"countries\": countries_found,\n",
        "                \"companies\": company_names\n",
        "            })\n",
        "\n",
        "        output_file = os.path.join(output_folder, f\"{file_name}.json\")\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as json_file:\n",
        "            json.dump(extracted_countries, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "        print(f\"Résultats enregistrés dans : {output_file}\")\n"
      ],
      "metadata": {
        "id": "t26JpqXGq3nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code 3 :  Extraction des infos à partir des textes en utilisant BERT**"
      ],
      "metadata": {
        "id": "yRSHkaSV7jRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load CamemBERT NER model\n",
        "ner_pipeline = pipeline(\"ner\", model=\"Jean-Baptiste/camembert-ner\", tokenizer=\"Jean-Baptiste/camembert-ner\", grouped_entities=True)\n",
        "\n",
        "company_keywords = [\n",
        "    \"Ltd\", \"Inc\", \"Corp\", \"LLC\", \"Company\", \"Corporation\", \"Enterprises\", \"Group\",\n",
        "    \"Holdings\", \"SARL\", \"SA\", \"SAS\", \"SNC\", \"Entreprise\", \"Compagnie\", \"Société\",\n",
        "    \"Groupe\", \"Office\", \"Gestion\", \"Opérations\"\n",
        "]\n",
        "\n",
        "title_keywords = [\n",
        "    \"contrat\", \"convention\", \"accord\", \"protocole\", \"entente\",\n",
        "    \"contract\", \"agreement\", \"memorandum\"\n",
        "]\n",
        "\n",
        "exclusion_keywords = [\"république\", \"ministère\", \"province\", \"direction\"]\n",
        "\n",
        "def clean_text(text):\n",
        "    return re.sub(r'\\s+', ' ', re.sub(r\"[^\\w\\s\\-’']\", '', text)).strip()\n",
        "\n",
        "def extract_title(text, max_lines=100):\n",
        "    lines = text.splitlines()\n",
        "\n",
        "    for i in range(min(len(lines), max_lines)):\n",
        "        line = lines[i].strip()\n",
        "\n",
        "        # Skip if not uppercase or too short\n",
        "        if not line.isupper() or len(line.split()) < 2:\n",
        "            continue\n",
        "\n",
        "        cleaned_line = clean_text(line)\n",
        "        lower_line = cleaned_line.lower()\n",
        "\n",
        "        # Must start with one of the main title keywords and not contain exclusions\n",
        "        if any(lower_line.startswith(kw) for kw in title_keywords):\n",
        "            if not any(excl in lower_line for excl in exclusion_keywords):\n",
        "                return line.strip()\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "\n",
        "def split_text(text, max_words=500):\n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i + max_words]) for i in range(0, len(words), max_words)]\n",
        "\n",
        "def is_company(name):\n",
        "    return any(re.search(r'\\b' + re.escape(kw) + r'\\b', name, re.IGNORECASE) for kw in company_keywords)\n",
        "\n",
        "def load_country_list(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return [line.strip() for line in f if line.strip()]\n",
        "\n",
        "text_folder = \"extracted_text\"\n",
        "output_folder = \"bert_output\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "country_list = load_country_list(\"countries_fr.txt\")\n",
        "\n",
        "for file_name in os.listdir(text_folder):\n",
        "    if not file_name.endswith(\".txt\"):\n",
        "        continue\n",
        "\n",
        "    with open(os.path.join(text_folder, file_name), 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    title = extract_title(content)\n",
        "    chunks = split_text(content)\n",
        "\n",
        "    ner_results = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        ner_out = ner_pipeline(chunk)\n",
        "        countries = set()\n",
        "        operating_companies = set()\n",
        "        others = set()\n",
        "\n",
        "        for ent in ner_out:\n",
        "            word = ent['word']\n",
        "            label = ent['entity_group']\n",
        "            if label in [\"LOC\", \"MISC\"]:\n",
        "                if any(re.fullmatch(re.escape(c), word, re.IGNORECASE) for c in country_list):\n",
        "                    countries.add(word)\n",
        "            elif label == \"ORG\":\n",
        "                if is_company(word):\n",
        "                    operating_companies.add(word)\n",
        "                else:\n",
        "                    others.add(word)\n",
        "\n",
        "        ner_results.append({\n",
        "            \"chunk\": i + 1,\n",
        "            \"Pays Cible\": sorted(countries),\n",
        "            \"Société d'exploitation\": sorted(operating_companies),\n",
        "            \"Autres sociétés\": sorted(others)\n",
        "        })\n",
        "\n",
        "    result = {\n",
        "        \"Titre Contrat\": title,\n",
        "        \"ner_chunks\": ner_results\n",
        "    }\n",
        "\n",
        "    output_filename = os.path.splitext(file_name)[0] + \".json\"\n",
        "    with open(os.path.join(output_folder, output_filename), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(result, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"BERT extracted data saved for {file_name}\")\n"
      ],
      "metadata": {
        "id": "szl0gLav7iDx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}