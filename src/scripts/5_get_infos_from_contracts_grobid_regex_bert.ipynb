{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install git+https://github.com/kermitt2/grobid_client_python.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install grobid-client-python\n",
    "!pip install requests\n",
    "!pip install ocrmypdf\n",
    "!pip install PyPDF2\n",
    "!pip install grobid-client\n",
    "!pip install beautifulsoup4\n",
    "!pip install PyMuPDF\n",
    "!pip install tiktoken protobuf\n",
    "!pip install sentencepiece\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install transformers torch\n",
    "!pip install accelerate\n",
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PDF Mining Contract Extraction Pipeline\n",
    "---------------------------------------\n",
    "This script processes scanned mining contracts in PDF format to extract\n",
    "structured information: contract title, country, company, and resources.\n",
    "\n",
    "It combines OCR (via GROBID), CamemBERT for NER, and fuzzy matching for company resolution.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import fitz  # PyMuPDF for reading PDFs\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import CamembertTokenizer, CamembertForTokenClassification, pipeline\n",
    "from collections import Counter\n",
    "from rapidfuzz import fuzz, process as rapidfuzz_process\n",
    "\n",
    "# --- File paths and model setup ---\n",
    "PDF_FOLDER = \"D:/MLAIM/S4/PDFs_Extraction/contrats_scannées1\"\n",
    "COUNTRY_LIST_FILE = \"D:/MLAIM/S4/PDFs_Extraction/countries_fr.txt\"\n",
    "RESOURCE_LIST_FILE = \"D:/MLAIM/S4/PDFs_Extraction/resources.txt\"\n",
    "COMPANY_LIST_FILE = \"D:/MLAIM/S4/PDFs_Extraction/societes.txt\"\n",
    "GROBID_URL = \"http://localhost:8070/api/processFulltextDocument\"\n",
    "OUTPUT_JSON = \"combined_extraction_output.json\"\n",
    "\n",
    "# --- Heuristic keyword lists ---\n",
    "company_keywords = [\n",
    "    \"Ltd\", \"Inc\", \"Corp\", \"LLC\", \"Company\", \"Corporation\", \"Enterprises\", \"Group\",\n",
    "    \"Holdings\", \"Services\", \"Solutions\", \"Industries\", \"Systems\", \"Technologies\",\n",
    "    \"Partners\", \"Consulting\", \"Management\", \"Trading\", \"Operations\",\n",
    "    \"SARL\", \"SA\", \"SAS\", \"SNC\", \"Entreprise\", \"Compagnie\", \"Société\", \"Groupe\",\n",
    "    \"Conseil\", \"Commerce\", \"Gestion\", \"Opérations\", \"Sprl\", \"Office\"\n",
    "]\n",
    "\n",
    "title_keywords = [\n",
    "    \"contrat\", \"convention\", \"accord\", \"protocole\", \"entente\",\n",
    "    \"contract\", \"agreement\", \"memorandum\"\n",
    "]\n",
    "\n",
    "exclusion_keywords = [\"république\", \"ministère\", \"province\", \"direction\"]\n",
    "\n",
    "# --- Load CamemBERT model for NER ---\n",
    "print(\"[Init] Loading CamemBERT NER model...\")\n",
    "model_name = \"Jean-Baptiste/camembert-ner\"\n",
    "tokenizer = CamembertTokenizer.from_pretrained(model_name)\n",
    "model = CamembertForTokenClassification.from_pretrained(model_name)\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n",
    "\n",
    "# --- Utility functions ---\n",
    "\n",
    "def fix_encoding(text: str) -> str:\n",
    "    \"\"\"Fix character encoding from GROBID output.\"\"\"\n",
    "    try:\n",
    "        return text.encode(\"latin1\").decode(\"utf-8\")\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "def load_list(file_path: str) -> list:\n",
    "    \"\"\"Load a list from a .txt file (one entry per line).\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove special characters and extra whitespace.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', re.sub(r\"[^\\w\\s\\-’']\", '', text)).strip()\n",
    "\n",
    "def extract_title_from_text(text: str) -> str:\n",
    "    \"\"\"Extract contract title heuristically from first page text.\"\"\"\n",
    "    lines = text.splitlines()\n",
    "    for line in lines[:100]:  # Only check first 100 lines\n",
    "        if not line.isupper() or len(line.split()) < 2:\n",
    "            continue\n",
    "        cleaned = clean_text(line).lower()\n",
    "        if any(cleaned.startswith(k) for k in title_keywords) and not any(e in cleaned for e in exclusion_keywords):\n",
    "            return line.strip()\n",
    "    return lines[0].strip() if lines else \"\"\n",
    "\n",
    "def extract_first_page_text(pdf_path: str) -> str:\n",
    "    \"\"\"Extract plain text from the first page of a PDF using PyMuPDF.\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        if len(doc) == 0:\n",
    "            return \"\"\n",
    "        return doc.load_page(0).get_text().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_keywords(text: str, keywords: list) -> list:\n",
    "    \"\"\"Return a list of keywords found in the text.\"\"\"\n",
    "    found = set()\n",
    "    text = text.lower()\n",
    "    for word in keywords:\n",
    "        if re.search(r'\\b' + re.escape(word) + r'\\b', text):\n",
    "            found.add(word)\n",
    "    return list(found)\n",
    "\n",
    "def split_text(text, max_words=500):\n",
    "    \"\"\"Split large text into chunks for NER processing.\"\"\"\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + max_words]) for i in range(0, len(words), max_words)]\n",
    "\n",
    "def is_company(name):\n",
    "    \"\"\"Check if a string resembles a company name using keyword presence.\"\"\"\n",
    "    return any(re.search(r'\\b' + re.escape(kw) + r'\\b', name, re.IGNORECASE) for kw in company_keywords)\n",
    "\n",
    "def match_to_known_company(entity, known_companies, threshold=85):\n",
    "    \"\"\"Fuzzy match an organization name to a known company list.\"\"\"\n",
    "    match, score, _ = rapidfuzz_process.extractOne(entity, known_companies, scorer=fuzz.token_sort_ratio)\n",
    "    return match if score >= threshold else None\n",
    "\n",
    "# --- Core extraction function ---\n",
    "\n",
    "def extract_contract_info(pdf_path: str, country_list: list, resource_list: list, company_list: list):\n",
    "    \"\"\"Process a single PDF file to extract structured contract information.\"\"\"\n",
    "    try:\n",
    "        # Step 1: OCR via GROBID\n",
    "        with open(pdf_path, \"rb\") as pdf_file:\n",
    "            response = requests.post(GROBID_URL, files={\"input\": pdf_file})\n",
    "        if response.status_code != 200:\n",
    "            print(f\"[Error] GROBID failed for {pdf_path}\")\n",
    "            return None\n",
    "\n",
    "        raw_xml = response.text\n",
    "        raw_text = fix_encoding(BeautifulSoup(raw_xml, \"lxml\").get_text())\n",
    "        first_page_text = extract_first_page_text(pdf_path)\n",
    "\n",
    "        # Step 2: Heuristic extraction\n",
    "        title = extract_title_from_text(first_page_text)\n",
    "        countries_heur = extract_keywords(raw_text, country_list)\n",
    "        resources_heur = extract_keywords(raw_text, resource_list)\n",
    "\n",
    "        # Step 3: Named Entity Recognition (NER)\n",
    "        country_counter = Counter()\n",
    "        company_matches = []\n",
    "        other_counter = Counter()\n",
    "\n",
    "        for chunk in split_text(raw_text):\n",
    "            ner_out = ner_pipeline(chunk)\n",
    "            for ent in ner_out:\n",
    "                word = ent['word'].strip()\n",
    "                label = ent['entity_group']\n",
    "                word_lower = word.lower()\n",
    "                if label in [\"LOC\", \"MISC\"] and word_lower in country_list:\n",
    "                    country_counter[word_lower] += 1\n",
    "                elif label == \"ORG\":\n",
    "                    matched = match_to_known_company(word_lower, company_list)\n",
    "                    if matched:\n",
    "                        company_matches.append(matched)\n",
    "                    elif is_company(word):\n",
    "                        other_counter[word] += 1\n",
    "\n",
    "        most_common_company = Counter(company_matches).most_common(2)\n",
    "        top_company = most_common_company[0][0] if most_common_company else \"\"\n",
    "\n",
    "        return {\n",
    "            \"id\": os.path.splitext(os.path.basename(pdf_path))[0].lower().strip(),\n",
    "            \"fichier\": os.path.basename(pdf_path),\n",
    "            \"titre_contrat\": title,\n",
    "            \"pays_cible\": sorted(set(countries_heur + list(country_counter.keys()))),\n",
    "            \"ressources\": resources_heur,\n",
    "            \"societe_exploitation\": top_company,\n",
    "            \"entites_principales\": {\n",
    "                \"pays\": list(country_counter.keys()),\n",
    "                \"societes_reconnues\": dict(Counter(company_matches)),\n",
    "                \"autres_orgs\": dict(other_counter),\n",
    "                \"societes_non_matchées\": [c for c in other_counter if match_to_known_company(c, company_list) is None]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] while processing {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main execution loop ---\n",
    "\n",
    "def main():\n",
    "    country_list = load_list(COUNTRY_LIST_FILE)\n",
    "    resource_list = load_list(RESOURCE_LIST_FILE)\n",
    "    company_list = load_list(COMPANY_LIST_FILE)\n",
    "    results = []\n",
    "\n",
    "    for filename in os.listdir(PDF_FOLDER):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            print(f\"\\n[Info] Processing {filename}...\")\n",
    "            path = os.path.join(PDF_FOLDER, filename)\n",
    "            data = extract_contract_info(path, country_list, resource_list, company_list)\n",
    "            if data:\n",
    "                results.append(data)\n",
    "\n",
    "    if results:\n",
    "        with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\n[Success] Extraction completed. Results saved to {OUTPUT_JSON}\")\n",
    "    else:\n",
    "        print(\"\\n[Warning] No data extracted.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Nom du fichier JSON contenant les données extraites\n",
    "EXTRACTED_JSON = \"combined_extraction_output.json\"\n",
    "\n",
    "# Chargement des données JSON\n",
    "with open(EXTRACTED_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Fonction pour extraire l'identifiant numérique depuis le champ \"id\" ou \"file_name\"\n",
    "def extract_numeric_id(entry): \n",
    "    id_str = entry.get(\"id\") or entry.get(\"file_name\", \"\")\n",
    "    id_str = id_str.replace(\".pdf\", \"\").strip().lower()  # Nettoyage du nom (suppression .pdf, espaces)\n",
    "    match = re.search(r'\\d+', id_str)  # Recherche du premier nombre dans le texte\n",
    "    return int(match.group()) if match else float(\"inf\")  # Retourne le nombre trouvé ou infini si aucun\n",
    "\n",
    "# Nettoyage et transformation du champ \"id\" pour chaque entrée\n",
    "for entry in data:\n",
    "    num_id = extract_numeric_id(entry)\n",
    "    entry[\"id\"] = num_id  # Remplace \"id\" textuel par l'identifiant numérique\n",
    "\n",
    "# Tri des données par identifiant numérique croissant\n",
    "sorted_data = sorted(data, key=lambda x: x[\"id\"])\n",
    "\n",
    "# Sauvegarde du fichier trié et nettoyé\n",
    "with open(\"sorted_by_numeric_id_cleaned.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sorted_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"[OK] Champs 'id' nettoyés et données triées. Enregistré dans 'sorted_by_numeric_id_cleaned.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from fuzzywuzzy import fuzz\n",
    "import unidecode\n",
    "\n",
    "# Chemins des fichiers\n",
    "EXTRACTED_JSON = \"sorted_by_numeric_id_cleaned.json\"       # Résultat de l’extraction automatique\n",
    "GROUND_TRUTH_XLSX = \"contracts_labelized_example_new.xlsx\" # Données de vérité terrain (annotations manuelles)\n",
    "\n",
    "# Chargement des données extraites depuis le fichier JSON\n",
    "with open(EXTRACTED_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    extracted_data = json.load(f)\n",
    "\n",
    "# Conversion des données JSON en DataFrame\n",
    "extracted_df = pd.DataFrame(extracted_data)\n",
    "\n",
    "# Chargement du fichier Excel contenant les labels manuels\n",
    "ground_truth_df = pd.read_excel(GROUND_TRUTH_XLSX)\n",
    "\n",
    "# Nettoyage des IDs pour les deux sources (on extrait uniquement le nombre)\n",
    "extracted_df[\"id\"] = extracted_df[\"id\"].astype(str).str.extract(r'(\\d+)')[0]\n",
    "ground_truth_df[\"id\"] = ground_truth_df[\"id\"].astype(str).str.extract(r'(\\d+)')[0]\n",
    "\n",
    "# On renomme les colonnes issues de l'extraction pour éviter la confusion avec les labels manuels\n",
    "extracted_df.rename(columns={\n",
    "    \"titre_contrat\": \"pred_titre_contrat\",\n",
    "    \"pays_cible\": \"pred_pays_cible\",\n",
    "    \"societe_exploitation\": \"pred_societe_exploitation\",\n",
    "    \"ressources\": \"pred_ressources\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Fusion des deux jeux de données sur l’ID\n",
    "merged_df = pd.merge(ground_truth_df, extracted_df, on=\"id\", how=\"inner\")\n",
    "print(f\"Found {len(merged_df)} matched rows\")\n",
    "\n",
    "# Si aucune correspondance, on arrête le script\n",
    "if merged_df.empty:\n",
    "    print(\"No matching IDs found.\")\n",
    "    exit()\n",
    "\n",
    "# --- FONCTIONS DE NORMALISATION --- #\n",
    "\n",
    "# Normalisation du texte (minuscules, sans accents, strip)\n",
    "def normalize_text(s):\n",
    "    if isinstance(s, str):\n",
    "        return unidecode.unidecode(s.lower().strip())\n",
    "    elif isinstance(s, list):\n",
    "        return [normalize_text(i) for i in s if isinstance(i, str)]\n",
    "    return s\n",
    "\n",
    "# Application de la normalisation sur les colonnes pertinentes\n",
    "for col in [\"titre_contrat\", \"pred_titre_contrat\",\n",
    "            \"societe_exploitation\", \"pred_societe_exploitation\",\n",
    "            \"pays_cible\", \"pred_pays_cible\",\n",
    "            \"ressources\", \"pred_ressources\"]:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df[col] = merged_df[col].apply(normalize_text)\n",
    "\n",
    "# --- MATCHING FLOU POUR LES CHAMPS TEXTUELS SIMPLES --- #\n",
    "\n",
    "# Fonction de comparaison floue basée sur le ratio de similarité (FuzzyWuzzy)\n",
    "def fuzzy_match(str1, str2, threshold=85):\n",
    "    if not isinstance(str1, str) or not isinstance(str2, str):\n",
    "        return False\n",
    "    return fuzz.token_set_ratio(str1, str2) >= threshold\n",
    "\n",
    "# Application du fuzzy matching sur les titres et les sociétés\n",
    "merged_df[\"titre_contrat_match\"] = merged_df.apply(\n",
    "    lambda row: fuzzy_match(row[\"titre_contrat\"], row[\"pred_titre_contrat\"]), axis=1)\n",
    "merged_df[\"societe_exploitation_match\"] = merged_df.apply(\n",
    "    lambda row: fuzzy_match(row[\"societe_exploitation\"], row[\"pred_societe_exploitation\"]), axis=1)\n",
    "\n",
    "# Affichage des scores d’accuracy flous\n",
    "print(f\"Accuracy (titre_contrat) with fuzzy matching: {merged_df['titre_contrat_match'].mean():.2f}\")\n",
    "print(f\"Accuracy (societe_exploitation) with fuzzy matching: {merged_df['societe_exploitation_match'].mean():.2f}\")\n",
    "\n",
    "# --- MATCHING MULTI-LABEL POUR LES PAYS --- #\n",
    "\n",
    "# Correction de certaines valeurs fréquentes de pays\n",
    "country_map = {\n",
    "    \"republique d'afrique du sud\": \"afrique du sud\",\n",
    "    \"iles vierges britanniques\": \"british virgin island\",\n",
    "    \"british virgin island\": \"british virgin island\",\n",
    "    \"rdc\": \"republique du congo\",\n",
    "    \"republique du congo\": \"republique du congo\",\n",
    "}\n",
    "\n",
    "# Conversion d’une string ou liste en liste nettoyée\n",
    "def to_list(x):\n",
    "    if isinstance(x, str):\n",
    "        return [i.strip() for i in x.split(\",\") if i.strip()]\n",
    "    elif isinstance(x, list):\n",
    "        return [str(i).strip() for i in x if isinstance(i, str)]\n",
    "    return []\n",
    "\n",
    "# Application du mapping\n",
    "def map_countries(lst):\n",
    "    return [country_map.get(c, c) for c in lst]\n",
    "\n",
    "# Préparation des colonnes pour le calcul multi-label\n",
    "merged_df[\"true_pays\"] = merged_df[\"pays_cible\"].apply(to_list).apply(normalize_text).apply(map_countries)\n",
    "merged_df[\"pred_pays\"] = merged_df[\"pred_pays_cible\"].apply(to_list).apply(normalize_text).apply(map_countries)\n",
    "\n",
    "# On filtre les lignes valides (non vides)\n",
    "valid_pays_df = merged_df[\n",
    "    merged_df[\"true_pays\"].apply(bool) & merged_df[\"pred_pays\"].apply(bool)\n",
    "]\n",
    "\n",
    "# Évaluation multi-label avec classification_report\n",
    "if valid_pays_df.empty:\n",
    "    print(\"\\nNo valid rows with non-empty 'pays_cible' and 'pred_pays_cible'.\")\n",
    "else:\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y_true = mlb.fit_transform(valid_pays_df[\"true_pays\"])\n",
    "    y_pred = mlb.transform(valid_pays_df[\"pred_pays\"])\n",
    "    support = y_true.sum(axis=0)\n",
    "\n",
    "    # On ne garde que les classes fréquentes (≥3 exemples)\n",
    "    min_support = 3\n",
    "    keep_indices = [i for i, count in enumerate(support) if count >= min_support]\n",
    "    class_names = [mlb.classes_[i] for i in keep_indices]\n",
    "\n",
    "    print(f\"\\nClassification Report for 'pays_cible' (filtered, ≥{min_support} samples):\")\n",
    "    if keep_indices:\n",
    "        y_true_filtered = y_true[:, keep_indices]\n",
    "        y_pred_filtered = y_pred[:, keep_indices]\n",
    "        print(classification_report(y_true_filtered, y_pred_filtered, target_names=class_names, zero_division=0))\n",
    "    else:\n",
    "        print(\"No classes with enough support to report.\")\n",
    "\n",
    "# --- ÉVALUATION DES RESSOURCES --- #\n",
    "\n",
    "if \"ressources\" in merged_df.columns and \"pred_ressources\" in merged_df.columns:\n",
    "    # Préparation des listes de ressources\n",
    "    merged_df[\"true_ressources\"] = merged_df[\"ressources\"].apply(to_list).apply(normalize_text)\n",
    "    merged_df[\"pred_ressources\"] = merged_df[\"pred_ressources\"].apply(to_list).apply(normalize_text)\n",
    "\n",
    "    valid_res_df = merged_df[\n",
    "        merged_df[\"true_ressources\"].apply(bool) & merged_df[\"pred_ressources\"].apply(bool)\n",
    "    ]\n",
    "\n",
    "    if valid_res_df.empty:\n",
    "        print(\"\\nNo valid rows with non-empty 'ressources' and 'pred_ressources'.\")\n",
    "    else:\n",
    "        mlb_res = MultiLabelBinarizer()\n",
    "        y_true_res = mlb_res.fit_transform(valid_res_df[\"true_ressources\"])\n",
    "        y_pred_res = mlb_res.transform(valid_res_df[\"pred_ressources\"])\n",
    "\n",
    "        print(\"\\nClassification Report for 'ressources':\")\n",
    "        print(classification_report(y_true_res, y_pred_res, target_names=mlb_res.classes_, zero_division=0))\n",
    "else:\n",
    "    print(\"\\nMissing 'ressources' or 'pred_ressources' columns in merged_df.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
